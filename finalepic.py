# -*- coding: utf-8 -*-
"""FinalEpic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10D-6gaRxhHHIR52-SVFPPmHP8mrr4MVA
"""

# importing important lib
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/Epics24/By Patel/modifiedData.csv')

data

column_to_drop = 'Timestamp'

# Drop the column
data = data.drop(column_to_drop, axis=1)

column_to_drop = 'What kind of mental health content would you like to see on social media?'

# Drop the column
data = data.drop(column_to_drop, axis=1)

column_to_drop = 'What kind of mental health support would you like to receive?'

# Drop the column
data = data.drop(column_to_drop, axis=1)

data

column_rename_mapping = {'Which age group do you belong to?': 'Age',
                         'Have you been experiencing any of these feelings and emotions in the past few months? [Frustration/Anger]': 'Feeling Anger/Frustration',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Fear/Panic]':'Feeling Fear/Panic',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Sadness]':'Feeling Sadness',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Loneliness]': 'Feeling Loneliness',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Helplessness]':'Feeling Helplessness',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Hopelessness]':'Feeling Hopelessness',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Regret/Guilt]':'Feeling Regret/Guilt',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Happiness/Joy]':' Feeling Happiness/ Joy',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Gratitute]':'Feeling Gratitude',
                        'Have you been experiencing any of these feelings and emotions in the past few months? [Secure]':'Feeling Secure'}

data = data.rename(columns=column_rename_mapping)

data

column_rename_mapping = {'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Mindfulness practices]': 'Mindfulness practices',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Reaching out to family and friends]':'Reaching out to family and friends',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Routine physical activity - Workout, Walking, Yoga, etc]':'Routine physical activity',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Reduced time on social media platforms]':'Reduced time on social media platforms',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Self-reflection/ Journaling/ Meditation/ Breathing exercises, etc]':'Self Reflection',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Binge watching - Netflix, Prime, Hotstar, etc]':'Binge watching',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Online games]':'Online games',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Over eating]':'Over eating',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Over sleeping]':'Over sleeping',
                        'If you have been stressed or experiencing any other unpleasant emotions, how have you been coping with them? [Withdrawing from loved ones]':'Withdrawing from loved ones',
                        'Would you be interested in joining a Sharing Circle?\nSharing circles are essentially safe spaces for having authentic conversations that will always be facilitated, with a professional if possible. i.e. Facilitators holding a space where others can share their thoughts & feelings without any expectations or judgements. It is a space where you feel safe & secure enough to be yourself & share anything.':'interested in joining a Sharing Circle?'}

data = data.rename(columns=column_rename_mapping)

data

# Encoding
# imports the LabelEncoder class from the sklearn.preprocessing module
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for column in data.columns:
  data[column]= le.fit_transform(data[column])

data

# chaking shape of the dataset(columns and row)
data.shape

#Chaking the datatypes of the all features
data.dtypes

column_to_drop = 'Are there any other feelings or emotions you have been experiencing in addition to the above?'
# Drop the column
data = data.drop(column_to_drop, axis=1)

column_to_drop = 'Would you like to elaborate on any of the feelings or emotions you have specified above?'
# Drop the column
data = data.drop(column_to_drop, axis=1)

data

#chaking the null values in the dataset
null_values_total =data.isnull().sum()

# Display the results
print("Total Null Values:")
print(null_values_total)

#chaking the statistic of our data
data.describe()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming your dataset is loaded into a DataFrame named 'df'
# Replace this with your actual dataset loading mechanism

# Scatter plot between 'Feeling Fear/Panic' and 'Online games'
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Feeling Fear/Panic', y='Online games', data=data)
plt.title('Scatter Plot: Feeling Fear/Panic vs. Online games')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='Feeling Fear/Panic', y='Online games', data=data)
plt.title('Box Plot: Feeling Fear/Panic vs. Online games')
plt.show()

#chaking the correlation between our output features and Feeling Fear/panic
correlation_fear = data['interested in joining a Sharing Circle?'].corr(data['Feeling Fear/Panic'])
print(f'Correlation with Feeling Fear/Panic: {correlation_fear}')

#chaking the correlation between our output features and sadness
correlation_sadness = data['interested in joining a Sharing Circle?'].corr(data['Feeling Sadness'])
print(f'Correlation with Feeling Sadness: {correlation_sadness}')

# Doing Ttest
# null hypothesis (H0) is a statement that there is no significant difference in the mean levels of "Feeling Fear/Panic" between individuals interested in
# joining a Sharing Circle and individuals not interested in joining a Sharing Circle. The alternative hypothesis (H 1) is the opposite of the null hypothesis
from scipy.stats import ttest_ind
interested = data[data['interested in joining a Sharing Circle?'] == 1]['Feeling Fear/Panic']
not_interested = data[data['interested in joining a Sharing Circle?'] == 0]['Feeling Fear/Panic']

t_stat, p_value = ttest_ind(interested, not_interested)
print(f'T-test for Feeling Fear/Panic: t_stat = {t_stat}, p_value = {p_value}')
# Result-If the p-value is less than the chosen significance level (e.g., 0.05), it would indicate that there is sufficient evidence to reject the null
# hypothesis, suggesting that there is a significant difference in the mean levels of "Feeling Fear/Panic" between the two groups.

# A positive t-statistic suggests that the mean of the 'interested' group (individuals interested in joining a Sharing Circle) is higher than the mean of
# the 'not_interested' group (individuals not interested in joining a Sharing Circle).



#0-> Low , 1-> High
data['interested in joining a Sharing Circle?'].value_counts()

#independent_variable
y =data.iloc[:,-1:]
print(y)

#dependent_variable
x =data.iloc[:,0:-1]
print(x)

#Splitting the Dataset: Training and Testing
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=1/3,random_state=0)

len(data)

len(x_train)

len(x_test)

from sklearn.linear_model import LogisticRegression #class
from sklearn.metrics import confusion_matrix #function
from sklearn.metrics import accuracy_score #function
from sklearn.metrics import precision_score #function
from sklearn.metrics import recall_score #function
from sklearn.metrics import f1_score #function

from sklearn.linear_model import LogisticRegression
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=1/3,random_state=0)

#Fitting simple logistic regression to the training test
Model1 = LogisticRegression()
Model1.fit(x_train, y_train)
#Predicting the test set results
prediction1 = Model1.predict(x_test)

prediction1

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
confusion_matrix(y_test,prediction1)

accuracy_score(y_test,prediction1)

# Train the model
Model1.fit(x_train, y_train)

# Predict on the training data
y_train_pred = Model1.predict(x_train)

# Calculate accuracy
accuracy = accuracy_score(y_train, y_train_pred)

# Print the accuracy
print(f"Training Accuracy: {accuracy}")

column_name=data.columns

column_name

target_variable = 'interested in joining a Sharing Circle?'

# Print the actual labels for the target variable
actual_labels = data[target_variable]

print("Actual Labels:")
print(actual_labels)

# Assuming 'actual_labels' is your actual labels array
# Assuming 'predicted_levels' is your predicted labels array

print("Length of Actual Labels:", len(actual_labels))
print("Length of Predicted Levels:", len(prediction1))

# Assuming 'actual_labels' is your actual labels array
# Assuming 'predicted_levels' is your predicted labels array

print("Length of Actual Labels:", len(actual_labels))
print("Length of Predicted Levels:", len(prediction1))

# Print additional information for debugging
print("Actual Labels:", actual_labels)
print("Predicted Levels:", prediction1)

# Assuming 'actual_labels' is your actual labels array
# Replace this with your actual dataset

# Check for null values
null_values = actual_labels.isnull().sum()

# Print the count of null values
print("Number of Null Values in Actual Labels:", null_values)

# Check for missing values (assuming that missing values are represented as NaN)
missing_values = actual_labels.isna().sum()

# Print the count of missing values
print("Number of Missing Values in Actual Labels:", missing_values)

Model1 = LogisticRegression(max_iter=1000)  # You can adjust the number of iterations as needed

#Fitting simple logistic regression to the training test
Model1 = LogisticRegression()
Model1.fit(x_train, y_train)
#Predicting the test set results
prediction1 = Model1.predict(x_test)

print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)
print("y_train shape:", y_train.shape)

import numpy as np
# Assuming y_train is a Python list->convert y_train list into numpy array
# y_train is a two-dimensional array (e.g., a column vector), ravel() will flatten it into a one-dimensional array.
y_train = np.array(y_train).ravel()

Model1 = LogisticRegression(max_iter=1000)
Model1.fit(x_train, y_train)
prediction1 = Model1.predict(x_test)

import numpy as np
# Assuming y_train is a 2D array with shape (69, 1)
# Convert it to a 1D array
y_train = np.ravel(y_train)
# Alternatively, you can use y_train = y_train.reshape(-1)

# Now, the shape of y_train should be (69,)
# Verify the shape
print("Shape of y_train:", y_train.shape)

# Now you can proceed with fitting the logistic regression model
Model1 = LogisticRegression(max_iter=1000)
Model1.fit(x_train, y_train)
prediction1 = Model1.predict(x_test)

from sklearn.tree import DecisionTreeClassifier

# Assuming x_train, y_train, and x_test are defined elsewhere in your code
tree = DecisionTreeClassifier(random_state=0, criterion="entropy")
tree.fit(x_train, y_train)

# Use 'tree' for prediction, not 'classifier'
prediction3 = tree.predict(x_test)

import numpy as np
# Assuming y_train is a Python list or a 2D array
y_train = np.array(y_train).ravel()

unique_labels = np.unique(y_train)
print("Unique Labels:", unique_labels)

Model1 = LogisticRegression(max_iter=1000)
Model1.fit(x_train, y_train)
prediction1 = Model1.predict(x_test)

prediction1

from sklearn.linear_model import LogisticRegression

# Assuming x_train, y_train, and x_test are your training features, training labels, and test features, respectively

# Fitting logistic regression model with increased max_iter
Model1 = LogisticRegression(max_iter=1000)
Model1.fit(x_train, y_train)

# Predicting the test set results
prediction1 = Model1.predict(x_test)

prediction1

# Assuming 'actual_labels' is your actual labels array
# Assuming 'predicted_levels' is your predicted labels array

print("Length of Actual Labels:", len(actual_labels))
print("Length of Predicted Levels:", len(prediction1))



# Print additional information for debugging
print("Actual Labels Indices:", range(len(actual_labels)))
print("Predicted Levels Indices:", range(len(prediction1)))

# Assuming 'x_test' is your test feature matrix
print("Length of x_test:", len(x_test))

# Reapply the model to the test set for predictions
prediction1 = Model1.predict(x_test)

# Print the length of the new predictions
print("Length of Predicted Levels:", len(prediction1))

from sklearn.metrics import accuracy_score

# Assuming 'actual_labels' is a DataFrame or Series with indices corresponding to the test set
# You might need to adjust this based on your actual data structure
actual_labels_for_test = actual_labels.iloc[:35]

# Calculate accuracy
accuracy = accuracy_score(actual_labels_for_test, prediction1)
print("Accuracy:", accuracy)

from sklearn.tree import DecisionTreeClassifier

# Assuming x_train, y_train, and x_test are defined elsewhere in your code
tree = DecisionTreeClassifier(random_state=0, criterion="entropy")
tree.fit(x_train, y_train)

# Use 'tree' for prediction, not 'classifier'
prediction3 = tree.predict(x_test)

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(random_state = 0,criterion = "entropy")
tree.fit(x_train, y_train)
prediction3 = tree.predict(x_test)

prediction3

from sklearn.metrics import precision_score, recall_score, f1_score

# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively
accuracy = accuracy_score(y_test, prediction3)
precision = precision_score(y_test, prediction3)
recall = recall_score(y_test, prediction3)
f1 = f1_score(y_test, prediction3)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)

confusion_matrix(y_test,prediction3)

import matplotlib. pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_true = y_test, y_pred = prediction3)
#plot_confusion_matrix(cm,level,title = "confusion_matrix")
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

#Support Vector Machine
from sklearn.ensemble import BaggingClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
svm = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9,probability=True),n_jobs=-1))
svm.fit(x_train, y_train)
prediction4 = svm.predict(x_test)

from sklearn.metrics import precision_score, recall_score, f1_score

# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively
accuracy = accuracy_score(y_test, prediction4)
precision = precision_score(y_test, prediction4)
recall = recall_score(y_test, prediction4)
f1 = f1_score(y_test, prediction4)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)

cm = confusion_matrix(y_true = y_test, y_pred = prediction4)
#plot_confusion_matrix(cm,level,title = "confusion_matrix")
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

from sklearn.naive_bayes import GaussianNB
nbcla = GaussianNB()
nbcla.fit(x_train, y_train)
prediction5 = nbcla.predict(x_test)

from sklearn.metrics import precision_score, recall_score, f1_score

# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively
accuracy = accuracy_score(y_test, prediction5)
precision = precision_score(y_test, prediction5)
recall = recall_score(y_test, prediction5)
f1 = f1_score(y_test, prediction5)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)

cm = confusion_matrix(y_true = y_test, y_pred = prediction5)
#plot_confusion_matrix(cm,level,title = "confusion_matrix")
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()



from sklearn.ensemble import RandomForestClassifier

# Initialize the classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model using training dataset
rf_classifier.fit(x_train, y_train)

# Make predictions on test dataset
prediction6 = rf_classifier.predict(x_test)

# Evaluate the accuracy of the model
#accuracy = rf_classifier.score(x_test, y_test)
#print("Accuracy:", accuracy)

from sklearn.metrics import precision_score, recall_score, f1_score

# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively
accuracy = accuracy_score(y_test, prediction6)
precision = precision_score(y_test, prediction6)
recall = recall_score(y_test, prediction6)
f1 = f1_score(y_test, prediction6)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 score:", f1)

cm = confusion_matrix(y_true = y_test, y_pred = prediction6)
#plot_confusion_matrix(cm,level,title = "confusion_matrix")
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()



#Finding Correlation
cn=data.corr()
cn

#Correlation
cmap=sns.diverging_palette(260,-10,s=50, l=75, n=6,as_cmap=True)
plt.subplots(figsize=(30,30))
sns.heatmap(cn,cmap="Blues",annot=True, square=True)
plt.show()

num_list = list(data.columns)

fig = plt.figure(figsize=(10,30))

for i in range(len(num_list)):
    plt.subplot(12,2,i+1)
    plt.title(num_list[i])
    plt.xticks(rotation=45)
    plt.hist(data[num_list[i]],color='blue',alpha=0.5)

plt.tight_layout()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor

# Assuming you have X_train, X_test, y_train, y_test
from sklearn.neighbors import KNeighborsRegressor
knn_reg = KNeighborsRegressor()
knn_reg.fit(x_train, y_train)
knn_reg.fit(x_train, y_train)
knn_pred = knn_reg.predict(x_test)
# Instantiate regression models
lr_reg = LinearRegression()
knn_reg = KNeighborsRegressor()
svr_reg = SVR()
dt_reg = DecisionTreeRegressor()
rf_reg = RandomForestRegressor()
gb_reg = GradientBoostingRegressor()
ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=500)

# Fit the regression models
lr_reg.fit(x_train, y_train)
knn_reg.fit(x_train, y_train)
svr_reg.fit(x_train, y_train)
dt_reg.fit(x_train, y_train)
rf_reg.fit(x_train, y_train)
gb_reg.fit(x_train, y_train)
ada_reg.fit(x_train, y_train)

# Make predictions
lr_pred = lr_reg.predict(x_test)
knn_pred = knn_reg.predict(x_test)
svr_pred = svr_reg.predict(x_test)
dt_pred = dt_reg.predict(x_test)
rf_pred = rf_reg.predict(x_test)
gb_pred = gb_reg.predict(x_test)
ada_pred = ada_reg.predict(x_test)

# Calculate regression metrics
lr_mae = mean_absolute_error(y_test, lr_pred)
knn_mae = mean_absolute_error(y_test, knn_pred)
svr_mae = mean_absolute_error(y_test, svr_pred)
dt_mae = mean_absolute_error(y_test, dt_pred)
rf_mae = mean_absolute_error(y_test, rf_pred)
gb_mae = mean_absolute_error(y_test, gb_pred)
ada_mae = mean_absolute_error(y_test, ada_pred)

lr_mse = mean_squared_error(y_test, lr_pred)
knn_mse = mean_squared_error(y_test, knn_pred)
svr_mse = mean_squared_error(y_test, svr_pred)
dt_mse = mean_squared_error(y_test, dt_pred)
rf_mse = mean_squared_error(y_test, rf_pred)
gb_mse = mean_squared_error(y_test, gb_pred)
ada_mse = mean_squared_error(y_test, ada_pred)
lr_r2 = r2_score(y_test, lr_pred)
knn_r2 = r2_score(y_test, knn_pred)
svr_r2 = r2_score(y_test, svr_pred)
dt_r2 = r2_score(y_test, dt_pred)
rf_r2 = r2_score(y_test, rf_pred)
gb_r2 = r2_score(y_test, gb_pred)
ada_r2 = r2_score(y_test, ada_pred)





import matplotlib.pyplot as plt

# List of algorithm names and their respective accuracy scores
algorithm_names = ["Logistic Regression", "KNN", "SVM", "Decision Tree", "Random Forest", "Gradient Boosting", "AdaBoost"]
accuracy_scores = [0.7375, 0.625, 0.7225, 0.635, 0.78, 0.7625, 0.8325]

# Create a histogram
plt.figure(figsize=(10, 6))
plt.bar(algorithm_names, accuracy_scores, color='blue')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Algorithms')
plt.ylim(0, 1)  # Set the y-axis limit from 0 to 1
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# List of algorithm names
algorithm_names = ["Logistic Regression", "KNN", "SVM", "Decision Tree", "Random Forest", "Gradient Boosting", "AdaBoost"]

# Precision, recall, and F1-score values for each algorithm
precision_scores = [0.753, 0.758, 0.766, 0.76, 0.814, 0.871, 0.836]
recall_scores = [0.967, 0.833, 0.983, 0.633, 0.95, 0.9, 0.933]
f1_scores = [0.847, 0.794, 0.861, 0.691, 0.877, 0.885, 0.882]

x = np.arange(len(algorithm_names))  # the label locations

width = 0.3  # the width of the bars

fig, ax = plt.subplots(figsize=(12, 6))
rects1 = ax.bar(x - width, precision_scores, width, label='Precision')
rects2 = ax.bar(x, recall_scores, width, label='Recall')
rects3 = ax.bar(x + width, f1_scores, width, label='F1-Score')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_xlabel('Algorithms')
ax.set_title('Metrics Comparison for Different Algorithms')
ax.set_xticks(x)
ax.set_xticklabels(algorithm_names, rotation=45, ha="right")
ax.legend()

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
autolabel(rects3)

fig.tight_layout()

plt.show()

data.dtypes

# Select relevant features and target variable
features = ['Occupation', 'Gender', 'Feeling Anger/Frustration', 'Feeling Fear/Panic',
            'Feeling Sadness', 'Feeling Loneliness', 'Feeling Helplessness', 'Feeling Hopelessness']
target = ['interested in joining a Sharing Circle?']

# Split the data into features and target variable
X = data[features]
y = data[target]

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Train the SVM model
svm_model = SVC(C=10, kernel='rbf', random_state=9, probability=True)
svm_model.fit(x_train_scaled, y_train)

# Function to preprocess user input
def preprocess_input(user_input):
    preprocessed_input = [user_input[feature] for feature in features]
    return preprocessed_input

# Function to predict joining a Sharing Circle based on user input
def predict_joining_sharing_circle(user_input):
    preprocessed_input = preprocess_input(user_input)
    prediction = svm_model.predict([preprocessed_input])
    return prediction[0]

# Take user input
user_input = {}
user_input['Occupation'] = int(input("Enter Occupation (0 for Student, 1 for Employed, 2 for Retired): "))
user_input['Gender'] = int(input("Enter Gender (0 for Male, 1 for Female, 2 for Prefer not to say): "))
user_input['Feeling Anger/Frustration'] = int(input("Enter Feeling Anger/Frustration (0 or 1): "))
user_input['Feeling Fear/Panic'] = int(input("Enter Feeling Fear/Panic (0 or 1): "))
user_input['Feeling Sadness'] = int(input("Enter Feeling Sadness (0 or 1): "))
user_input['Feeling Loneliness'] = int(input("Enter Feeling Loneliness (0 or 1): "))
user_input['Feeling Helplessness'] = int(input("Enter Feeling Helplessness (0 or 1): "))
user_input['Feeling Hopelessness'] = int(input("Enter Feeling Hopelessness (0 or 1): "))

# Make predictions
prediction = predict_joining_sharing_circle(user_input)

# Output the prediction
if prediction == 1:
    print("Based on the input, the person is likely interested in joining a Sharing Circle.")
else:
    print("Based on the input, the person is not likely interested in joining a Sharing Circle.")

data.columns

# Select relevant features and target variable
features = ['Occupation', 'Gender', 'Feeling Anger/Frustration', 'Feeling Fear/Panic',
            'Feeling Sadness', 'Feeling Loneliness', 'Feeling Helplessness', 'Feeling Hopelessness']
target = ['interested in joining a Sharing Circle?']

# Split the data into features and target variable
X = data[features]
y = data[target]

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Train the SVM model
svm_model = SVC(C=10, kernel='rbf', random_state=9, probability=True)
svm_model.fit(x_train_scaled, y_train)

# Function to preprocess user input
def preprocess_input(user_input):
    preprocessed_input = [user_input[feature] for feature in features]
    return preprocessed_input

# Function to predict joining a Sharing Circle based on user input
def predict_joining_sharing_circle(user_input):
    preprocessed_input = preprocess_input(user_input)
    prediction = svm_model.predict([preprocessed_input])
    return prediction[0]

# Take user input
user_input = {}
user_input['Occupation'] = int(input("Enter Occupation (0 for Student, 1 for Employed, 2 for Retired): "))
user_input['Gender'] = int(input("Enter Gender (0 for Male, 1 for Female, 2 for Prefer not to say): "))
user_input['Feeling Anger/Frustration'] = int(input("Enter Feeling Anger/Frustration (0 or 1): "))
user_input['Feeling Fear/Panic'] = int(input("Enter Feeling Fear/Panic (0 or 1): "))
user_input['Feeling Sadness'] = int(input("Enter Feeling Sadness (0 or 1): "))
user_input['Feeling Loneliness'] = int(input("Enter Feeling Loneliness (0 or 1): "))
user_input['Feeling Helplessness'] = int(input("Enter Feeling Helplessness (0 or 1): "))
user_input['Feeling Hopelessness'] = int(input("Enter Feeling Hopelessness (0 or 1): "))

# Make predictions
prediction = predict_joining_sharing_circle(user_input)

# Output the prediction
if prediction == 1:
    print("Based on the input, the person is likely interested in joining a Sharing Circle.")
else:
    print("Based on the input, the person is not likely interested in joining a Sharing Circle.")

import pandas as pd
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# Select relevant features and target variables
features = ['Occupation', 'Gender', 'Feeling Anger/Frustration', 'Feeling Fear/Panic',
            'Feeling Sadness', 'Feeling Loneliness', 'Feeling Helplessness', 'Feeling Hopelessness']
targets = ['Reaching out to family and friends', 'Routine physical activity', 'Reduced time on social media platforms']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[features])

# Train a separate SVM model for each target variable
svm_models = {}
for target in targets:
    y = data[target]
    x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    svm_model = SVC(C=10, kernel='rbf', random_state=9, probability=True)
    svm_model.fit(x_train, y_train)
    svm_models[target] = svm_model

# Function to preprocess user input
def preprocess_input(user_input):
    preprocessed_input = [user_input[feature] for feature in features]
    return preprocessed_input

# Function to predict feedback based on user input
def predict_feedback(user_input, target):
    preprocessed_input = preprocess_input(user_input)
    prediction = svm_models[target].predict([preprocessed_input])
    return prediction[0]

# Take user input
user_input = {}
user_input['Occupation'] = int(input("Enter Occupation (0 for Student, 1 for Employed, 2 for Retired): "))
user_input['Gender'] = int(input("Enter Gender (0 for Male, 1 for Female, 2 for Prefer not to say): "))
user_input['Feeling Anger/Frustration'] = int(input("Enter Feeling Anger/Frustration (0 or 1): "))
user_input['Feeling Fear/Panic'] = int(input("Enter Feeling Fear/Panic (0 or 1): "))
user_input['Feeling Sadness'] = int(input("Enter Feeling Sadness (0 or 1): "))
user_input['Feeling Loneliness'] = int(input("Enter Feeling Loneliness (0 or 1): "))
user_input['Feeling Helplessness'] = int(input("Enter Feeling Helplessness (0 or 1): "))
user_input['Feeling Hopelessness'] = int(input("Enter Feeling Hopelessness (0 or 1): "))

# Make predictions for each feedback category
for target in targets:
    prediction = predict_feedback(user_input, target)
    if prediction == 1:
        print(f"The person is likely to provide positive feedback for {target}.")
    else:
        print(f"The person is not likely to provide positive feedback for {target}.")

import pandas as pd
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# Select relevant features and target variables
features = ['Occupation', 'Gender', 'Feeling Anger/Frustration', 'Feeling Fear/Panic',
            'Feeling Sadness', 'Feeling Loneliness', 'Feeling Helplessness', 'Feeling Hopelessness']
additional_targets = ['Self Reflection', 'Binge watching', 'Online games',
                      'Over eating', 'Over sleeping', 'Withdrawing from loved ones']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[features])

# Train a separate SVM model for each target variable
svm_models = {}
for target in additional_targets:
    y = data[target]
    x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    svm_model = SVC(C=10, kernel='rbf', random_state=9, probability=True)
    svm_model.fit(x_train, y_train)
    svm_models[target] = svm_model

# Function to preprocess user input
def preprocess_input(user_input):
    preprocessed_input = [user_input[feature] for feature in features]
    return preprocessed_input

# Function to predict feedback based on user input
def predict_feedback(user_input, target):
    preprocessed_input = preprocess_input(user_input)
    prediction = svm_models[target].predict([preprocessed_input])
    return prediction[0]

# Take user input
user_input = {}
user_input['Occupation'] = int(input("Enter Occupation (0 for Student, 1 for Employed, 2 for Retired): "))
user_input['Gender'] = int(input("Enter Gender (0 for Male, 1 for Female, 2 for Prefer not to say): "))
user_input['Feeling Anger/Frustration'] = int(input("Enter Feeling Anger/Frustration (0 or 1): "))
user_input['Feeling Fear/Panic'] = int(input("Enter Feeling Fear/Panic (0 or 1): "))
user_input['Feeling Sadness'] = int(input("Enter Feeling Sadness (0 or 1): "))
user_input['Feeling Loneliness'] = int(input("Enter Feeling Loneliness (0 or 1): "))
user_input['Feeling Helplessness'] = int(input("Enter Feeling Helplessness (0 or 1): "))
user_input['Feeling Hopelessness'] = int(input("Enter Feeling Hopelessness (0 or 1): "))

# Make predictions for each feedback category
for target in additional_targets:
    prediction = predict_feedback(user_input, target)
    if prediction == 1:
        print(f"The person is likely to provide positive feedback for {target}.")
    else:
        print(f"The person is not likely to provide positive feedback for {target}.")